{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTZh/QjuJStWybyARvS3iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitMishka/1/blob/main/MIMO_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaycsVa8sUOn",
        "outputId": "3e47e08e-ba16-4750-b9a0-de7f3c9f422d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-cb17b3e177cd>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_moveout['Move Out\\nDate'] = pd.to_datetime(df_moveout['Move Out\\nDate'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import files\n",
        "\n",
        "# Load the CSV file\n",
        "file_path_moveout = 'moveout.csv'  # Adjust this path to match the location of your uploaded file\n",
        "df_moveout = pd.read_csv(file_path_moveout)\n",
        "\n",
        "# Initial population of 'Community' column based on the first column's \"Community:\" markers\n",
        "df_moveout['Community'] = None\n",
        "current_community = None\n",
        "for index, row in df_moveout.iterrows():\n",
        "    if str(row.iloc[0]).startswith('Community:'):\n",
        "        current_community = row.iloc[0].split(':', 1)[1].strip()\n",
        "    else:\n",
        "        df_moveout.at[index, 'Community'] = current_community\n",
        "\n",
        "# Now refine 'Community' column values by applying LEFT(RIGHT(community,5),4) logic\n",
        "df_moveout['Community'] = df_moveout['Community'].apply(lambda x: x[-5:-1] if x is not None else None)\n",
        "\n",
        "# Continue with the rest of the processing\n",
        "# Assuming 'Resident' is correctly named and dropping rows without a Resident value\n",
        "df_moveout = df_moveout.dropna(subset=['Resident '])\n",
        "df_moveout['Move Out\\nDate'] = pd.to_datetime(df_moveout['Move Out\\nDate'], errors='coerce')\n",
        "\n",
        "# if pd.api.types.is_datetime64_any_dtype(df_moveout['Move Out\\nDate']):\n",
        "#     # Calculate the previous month and year for filtering\n",
        "#     today = datetime.now()\n",
        "#     first_day_of_current_month = today.replace(day=1)\n",
        "#     last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)\n",
        "#     previous_month = last_day_of_previous_month.month\n",
        "#     previous_month_year = last_day_of_previous_month.year\n",
        "\n",
        "#     df_moveout = df_moveout[(df_moveout['Move Out\\nDate'].dt.month == previous_month) & (df_moveout['Move Out\\nDate'].dt.year == previous_month_year)]\n",
        "# else:\n",
        "#     print(\"Error: Move Out Date column is not in datetime format.\")\n",
        "\n",
        "\n",
        "\n",
        "# List of values in 'Resident ' column that should lead to row deletion\n",
        "values_to_delete = [\n",
        "    \"Morning Pointe of Athens (aths)\", \"Death-At Community\", \"Death-Outside Community\", \"Financial-Competitor\", \"Move Out \\nReason\", \"Move-out Analysis \\nby Move Out Reason\",\n",
        "    \"Financial-Family Home\", \"Health improved\", \"Higher LOC-Nursing Home\", \"Relocating to other area\",\n",
        "    \"Morning Pointe of Brentwood (brwd)\", \"Discharged to Family\", \"Evicted-Financial\", \"Higher LOC-Hospice\",\n",
        "    \"Higher LOC-Memory Care\",\n",
        "    \"Morning Pointe of Calhoun (calh)\", \"Dissatisfied-Competitor\", \"Evicted-Behavior\", \"Evicted-Medical\",\n",
        "    \"Morning Pointe of Chattanooga Shallowford (chtt)\", \"Dissatisfied-Family Home\", \"Financial-Own Home\",\n",
        "    \"Transfer to Sister MP\",\n",
        "    \"Morning Pointe of Clinton (clin)\", \"Higher LOC-Geri/Psych\",\n",
        "    \"Morning Pointe of Columbia (colm)\",\n",
        "    \"Morning Pointe of Danville (danv)\",\n",
        "    \"Morning Pointe of East Hamilton (eham)\", \"Dissatisfied-Own Home\",\n",
        "    \"Morning Pointe of Frankfort (frkt)\",\n",
        "    \"Morning Pointe of Franklin (frln)\",\n",
        "    \"Morning Pointe of Franklin TN (fktn)\",\n",
        "    \"Morning Pointe of Greenbriar (grnb)\",\n",
        "    \"Morning Pointe of Greeneville (grnv)\",\n",
        "    \"Morning Pointe of Happy Valley (hppy)\",\n",
        "    \"Morning Pointe of Hardin Valley (hard)\", \"Natural Disaster - Moved to Competitor\",\n",
        "    \"Morning Pointe of Hixson (hixn)\",\n",
        "    \"Morning Pointe of Knoxville (knox)\",\n",
        "    \"Morning Pointe of Lenoir City (lenc)\",\n",
        "    \"Morning Pointe of Lexington (lexn)\", \"End of Respite\",\n",
        "    \"Morning Pointe of Lexington-East (lexe)\",\n",
        "    \"Morning Pointe of Louisville (lvlm)\",\n",
        "    \"Morning Pointe of Powell (powl)\",\n",
        "    \"Morning Pointe of Richmond (rich)\",\n",
        "    \"Morning Pointe of Russell (russ)\",\n",
        "    \"Morning Pointe of Spring Hill (sprh)\",\n",
        "    \"Morning Pointe of Tullahoma (tula)\",\n",
        "    \"Morning Pointe of Tuscaloosa (tusc)\",\n",
        "    \"The Lantern at Morning Pointe of Chattanooga (chtl)\",\n",
        "    \"The Lantern at Morning Pointe of Collegedale (cgdl)\",\n",
        "    \"The Lantern at Morning Pointe of Frankfort (frkl)\",\n",
        "    \"The Lantern at Morning Pointe of Franklin TN (fktl)\",\n",
        "    \"The Lantern at Morning Pointe of Knoxville (knxl)\",\n",
        "    \"The Lantern at Morning Pointe of Lenoir City (lenl)\",\n",
        "    \"The Lantern at Morning Pointe of Lexington (lexl)\",\n",
        "    \"The Lantern at Morning Pointe of Louisville (lvll)\",\n",
        "    \"The Lantern at Morning Pointe of Powell (pwll)\",\n",
        "    \"The Lantern at Morning Pointe of Russell (rusl)\",\n",
        "    \"The Lantern at Morning Pointe of Spring Hill (sprl)\"\n",
        "]\n",
        "\n",
        "\n",
        "# Filter out rows where 'Resident ' column contains any of the specified values\n",
        "df_moveout = df_moveout[~df_moveout['Resident '].isin(values_to_delete)]\n",
        "\n",
        "# Continue with the rest of the processing\n",
        "\n",
        "\n",
        "# Defer deletion of specified columns until after all other processing\n",
        "df_moveout = df_moveout.loc[:, ~df_moveout.columns.str.contains('^Unnamed')]\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Save the processed DataFrame to a new CSV file\n",
        "output_filename = f'moveout_summary_{timestamp}.csv'\n",
        "df_moveout.to_csv(output_filename, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load the data\n",
        "file_path = 'movein.csv'  # Adjust this to your file's path\n",
        "df_movein = pd.read_csv(file_path)\n",
        "\n",
        "# Your existing data processing steps...\n",
        "\n",
        "# Convert dates from string to datetime\n",
        "df_movein['Service Start\\nDate'] = pd.to_datetime(df_movein['Service Start\\nDate'], errors='coerce')\n",
        "df_movein['Move In\\nDate'] = pd.to_datetime(df_movein['Move In\\nDate'], errors='coerce')\n",
        "\n",
        "# Assume 'server_start_date_str' is the server start date in 'YYYY-MM-DD' format\n",
        "server_start_date_str = '2024-03-01'  # Example server start date\n",
        "server_start_date = datetime.strptime(server_start_date_str, '%Y-%m-%d')\n",
        "\n",
        "# Calculate the date 3 months before the server start date\n",
        "three_months_prior = server_start_date - timedelta(days=90)\n",
        "\n",
        "# Filter the DataFrame to include only rows where the 'Service Start\\nDate' is within the last three months before the server start date\n",
        "df_movein = df_movein[(df_movein['Service Start\\nDate'] >= three_months_prior) & (df_movein['Service Start\\nDate'] < server_start_date)]\n",
        "\n",
        "\n",
        "# Remove the first 10 rows of the DataFrame\n",
        "df_movein = df_movein.iloc[10:].reset_index(drop=True)\n",
        "\n",
        "# Create a timestamp for the filename\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "df_movein = df_movein.loc[:, ~df_movein.columns.str.contains('Unnamed')]  # NEW LINE TO DELETE UNNAMED COLUMNS\n",
        "\n",
        "# Save the cleaned and updated dataframe to a new CSV with timestamp in filename\n",
        "filename = f'movein_summary_{timestamp}.csv'\n",
        "df_movein.to_csv(filename, index=False)\n",
        "\n",
        "# Display the filename for confirmation\n",
        "#print(f\"File saved as: {filename}\")\n",
        "\n",
        "# Count the number of entries per region and display the counts\n",
        "# weights_sum_by_region = df_movein.groupby('Region')['Weights'].sum()\n",
        "# print(weights_sum_by_region)\n"
      ],
      "metadata": {
        "id": "hmLhbbqnsibc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}